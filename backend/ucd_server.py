"""UCD-sever.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E2UqUssxm1ouwbwJL_IxvNAXbC3HhQoD
"""

from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling
from torch.utils.data import Dataset
import torch
import re
import os

# ======================
# CONFIGURATION
# ======================
MODEL_NAME = "distilgpt2"  # Lightweight model
DATASET_FILE = "ucdavis_data.txt"  # Your cleaned dataset
OUTPUT_DIR = "./ucdavis-model-production"  # New output folder
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ======================
# TOKENIZER SETUP (FIXED)
# ======================
tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token  # Critical fix
tokenizer.add_special_tokens({
    'additional_special_tokens': [
        '[LOC]', '[Q]', '[A]'  # Structured prompting
    ]
})

# ======================
# MODEL INITIALIZATION
# ======================
model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)
model.resize_token_embeddings(len(tokenizer))
model.to(DEVICE)
model.config.pad_token_id = tokenizer.eos_token_id  # Sync config

# ======================
# DATA LOADER (OPTIMIZED)
# ======================
class UCDavisDataset(Dataset):
    def __init__(self, file_path):
        self.examples = []
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith(('http', 'www')):  # Filter URLs
                    tokenized = tokenizer(
                        line,
                        max_length=96,
                        truncation=True,
                        padding='max_length',
                        return_tensors='pt'
                    )
                    tokenized['labels'] = tokenized['input_ids'].clone()
                    self.examples.append(tokenized)

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, idx):
        return {k: v.squeeze(0).to(DEVICE) for k, v in self.examples[idx].items()}

# ======================
# TRAINING SETUP
# ======================
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=40,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,
    learning_rate=2.5e-5,
    weight_decay=0.01,
    fp16=DEVICE == "cuda",
    logging_steps=50,
    save_steps=200,
    report_to="none",
    remove_unused_columns=False  # Important for custom datasets
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=UCDavisDataset(DATASET_FILE),
    data_collator=DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False  # Standard language modeling
    ),
)

# ======================
# TRAINING EXECUTION
# ======================
print("Starting training...")
trainer.train()
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
print("Training complete!")

# ======================
# GENERATION FUNCTION (FIXED)
# ======================
# ======================
# UPDATED GENERATION FUNCTION
# ======================
def generate_response(prompt):
    # 1. Verify device alignment first
    if next(model.parameters()).device != DEVICE:
        model.to(DEVICE)  # Force model to correct device
    
    # 2. Tokenize with device awareness
    input_text = f"[LOC] UC Davis [Q] {prompt} [A]"
    inputs = tokenizer(
        input_text, 
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=64
    )
    
    # 3. Explicit device movement (enhanced)
    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
    
    # 4. Generation with safety context
    with torch.no_grad():  # Critical for MPS/CUDA stability
        outputs = model.generate(
            **inputs,
            max_new_tokens=60,
            temperature=0.7,  # Added for better quality
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,  # Explicit stop
            do_sample=True,  # Enable probabilistic sampling
            top_p=0.9  # Nucleus sampling
        )
    
    # 5. Robust answer cleaning
    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    answer = full_text.split("[A]")[-1].strip()
    clean_answer = re.sub(r'\[.*?\]|\b(?:UC Davis|Q:|A:)\b', '', answer)
    return clean_answer.split(".")[0] + "."


# ======================
# TESTING
# ======================
'''
test_questions = [
    "Where can I find boba?",
    "Best quiet study spot?",
    "Cheap food options?",
    "Good group study places?",
    "Best study coffee shop?"
]

print("\nUC Davis Assistant - Production Ready:")
for q in test_questions:
    response = generate_response(q)
    print(response)
'''